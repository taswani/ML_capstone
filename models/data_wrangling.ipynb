{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the process by which I cleaned the data I am using for my capstone project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading csvs into respective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_csv = \"../data_csv/AMZN.csv\"\n",
    "headline_csv = \"../data_csv/combined_amazon_date_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for datetime conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_conversion(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df = df.dropna(subset=['Date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes in the file path of the data associated with the price and headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(price_csv, headline_csv):\n",
    "    # read in data\n",
    "    amazon_df = pd.read_csv(headline_csv, index_col=False)\n",
    "    amzn_df = pd.read_csv(price_csv, index_col=False)\n",
    "    # adjust datetime\n",
    "    datetime_conversion(amazon_df)\n",
    "    datetime_conversion(amzn_df)\n",
    "    # merging\n",
    "    amzn_df = amzn_df.set_index('Date').join(amazon_df.set_index('Date'))\n",
    "    result_df = amzn_df\n",
    "    # dates being sorted for forward fill\n",
    "    result_df = result_df.sort_values(by='Date')\n",
    "    # cover null values in data\n",
    "    result_df = result_df.fillna(method='ffill')\n",
    "    result_df = result_df.fillna(method='bfill')\n",
    "    # Changing to Dask dataframe in order to parallelize the data wrangling (use case for large datasets)\n",
    "    result_df = dd.from_pandas(result_df, npartitions=1)\n",
    "    # dropping any mention of anything other than the company amazon\n",
    "    to_drop = ['rainforest', 'forest', 'Brazil', 'river', 'jungle', 'River', 'pilots', 'gangs', 'drugs', 'OkCupid', 'dating']\n",
    "    result_df = result_df[~result_df['Headlines'].str.contains('|'.join(to_drop))]\n",
    "    # combining duplicate headlines by date\n",
    "    result_df = result_df.groupby(['Date', 'Open', 'High', 'Low', 'Close'])['Headlines'].apply('// '.join).reset_index()\n",
    "    # Changing Dask dataframe to Pandas for reindexing\n",
    "    result_df = result_df.compute()\n",
    "    # setting index as dates, and imputing missing dates\n",
    "    # Every value is forward filled as a necessity to keep everything in sync\n",
    "    result_df = result_df.set_index('Date')\n",
    "    idx = pd.date_range('2016-06-30', '2019-08-11')\n",
    "    result_df = result_df.reindex(idx)\n",
    "    result_df = result_df.fillna(method='ffill')\n",
    "    # Adding a rolling window mean\n",
    "    result_df['Average Mean'] = result_df[['Close']].rolling(window = 100).mean()\n",
    "    result_df = result_df.fillna(method='bfill')\n",
    "    # Adding a differential column\n",
    "    result_df['Differential'] = result_df['High'].values - result_df['Low'].values\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to vectorize headlines for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(result_df):\n",
    "    #Creation of corpus from all the headlines\n",
    "    corpus = []\n",
    "    for line in result_df['Headlines']:\n",
    "        corpus.append(line)\n",
    "    #preprocess text for vectorization\n",
    "    processed_features = []\n",
    "    for sentence in range(0, len(corpus)):\n",
    "        # Remove all the special characters\n",
    "        processed_feature = re.sub(r'[^a-zA-Z\\s]', '', corpus[sentence], re.I|re.A)\n",
    "        # remove all single characters\n",
    "        processed_feature = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "        # Remove single characters from the start\n",
    "        processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "        # Removing prefixed 'b'\n",
    "        processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "        # Converting to Lowercase\n",
    "        processed_feature = processed_feature.lower().strip()\n",
    "        processed_features.append(processed_feature)\n",
    "    #Main part of the data pipeline\n",
    "    # #Initializing Vectorizer\n",
    "    # vectorizer = TfidfVectorizer(stop_words = stopwords.words('english'))\n",
    "    # #Transforming words to vectors\n",
    "    # processed_features = vectorizer.fit_transform(processed_features).toarray()\n",
    "    # corpus_df = pd.DataFrame(processed_features, columns = vectorizer.get_feature_names())\n",
    "    return processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying dataframe after text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanuj/Documents/GitHub/ML_capstone/models/data_pipeline.py:48: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  result_df = result_df.groupby(['Date', 'Open', 'High', 'Low', 'Close'])['Headlines'].apply('// '.join).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date         datetime64[ns]\n",
      "Open                float64\n",
      "High                float64\n",
      "Low                 float64\n",
      "Close               float64\n",
      "Headlines            object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Average Mean</th>\n",
       "      <th>Differential</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Average Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>717.200012</td>\n",
       "      <td>719.369995</td>\n",
       "      <td>712.539978</td>\n",
       "      <td>715.619995</td>\n",
       "      <td>Amazon Inspire, a resource site where teachers...</td>\n",
       "      <td>768.580997</td>\n",
       "      <td>6.830017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>717.320007</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>716.539978</td>\n",
       "      <td>725.679993</td>\n",
       "      <td>Amazon Inspire, a resource site where teachers...</td>\n",
       "      <td>768.580997</td>\n",
       "      <td>11.460022</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>717.320007</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>716.539978</td>\n",
       "      <td>725.679993</td>\n",
       "      <td>Amazon Inspire, a resource site where teachers...</td>\n",
       "      <td>768.580997</td>\n",
       "      <td>11.460022</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-07-03</td>\n",
       "      <td>717.320007</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>716.539978</td>\n",
       "      <td>725.679993</td>\n",
       "      <td>Amazon Inspire, a resource site where teachers...</td>\n",
       "      <td>768.580997</td>\n",
       "      <td>11.460022</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>717.320007</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>716.539978</td>\n",
       "      <td>725.679993</td>\n",
       "      <td>Amazon Inspire, a resource site where teachers...</td>\n",
       "      <td>768.580997</td>\n",
       "      <td>11.460022</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close  \\\n",
       "2016-06-30  717.200012  719.369995  712.539978  715.619995   \n",
       "2016-07-01  717.320007  728.000000  716.539978  725.679993   \n",
       "2016-07-02  717.320007  728.000000  716.539978  725.679993   \n",
       "2016-07-03  717.320007  728.000000  716.539978  725.679993   \n",
       "2016-07-04  717.320007  728.000000  716.539978  725.679993   \n",
       "\n",
       "                                                    Headlines  Average Mean  \\\n",
       "2016-06-30  Amazon Inspire, a resource site where teachers...    768.580997   \n",
       "2016-07-01  Amazon Inspire, a resource site where teachers...    768.580997   \n",
       "2016-07-02  Amazon Inspire, a resource site where teachers...    768.580997   \n",
       "2016-07-03  Amazon Inspire, a resource site where teachers...    768.580997   \n",
       "2016-07-04  Amazon Inspire, a resource site where teachers...    768.580997   \n",
       "\n",
       "            Differential  Sentiment  Polarity  Average Polarity  \n",
       "2016-06-30      6.830017          0       0.0               0.0  \n",
       "2016-07-01     11.460022          0       0.0               0.0  \n",
       "2016-07-02     11.460022          0       0.0               0.0  \n",
       "2016-07-03     11.460022          0       0.0               0.0  \n",
       "2016-07-04     11.460022          0       0.0               0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text_classification import *\n",
    "\n",
    "result_df = sentiment_analysis(result_df, processed_features)\n",
    "result_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
